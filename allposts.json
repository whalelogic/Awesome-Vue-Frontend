[
  {
    "id": "1",
    "slug": "flask-rest-api-setup",
    "title": "Flask REST API Setup",
    "subtitle": "A Step-by-Step Tutorial",
    "author": "Keith Thomson",
    "content": "# üêç Flask REST API Setup: A Step-by-Step Tutorial\n\n## üìã Introduction\n\nIn this tutorial, we'll walk through setting up a **basic Flask REST API**. We'll cover the essential steps, from installing dependencies to creating routes for **CRUD operations**.\n\n**What you'll learn:**\n- üõ†Ô∏è Setting up Flask and Flask-RESTful\n- üìÇ Creating a Flask application\n- üîß Defining API endpoints\n- ‚ñ∂Ô∏è Running and testing your API\n\n---\n\n## üõ†Ô∏è Step 1: Install Dependencies\n\nTo start, you'll need to install **Flask** and **Flask-RESTful**. Use `pip` to install them:\n\n```bash\npip install flask flask-restful\n```\n\n---\n\n## üìÇ Step 2: Create a New Flask App\n\nCreate a new file called `app.py` and add the following code:\n\n```python\nfrom flask import Flask\nfrom flask_restful import Api\n\napp = Flask(__name__)\napi = Api(app)\n\n@app.route('/')\ndef home():\n    return \"Welcome to my API!\"\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\nThis sets up a basic Flask app with a single route.\n\n---\n\n## üîß Step 3: Define Your API Endpoints\n\nLet's create a simple API for managing books. We'll define endpoints for CRUD operations:\n\n```python\nfrom flask_restful import Resource, reqparse\n\n# Sample in-memory data store\nbooks = [\n    {\"id\": 1, \"title\": \"Book 1\", \"author\": \"Author 1\"},\n    {\"id\": 2, \"title\": \"Book 2\", \"author\": \"Author 2\"}\n]\n\nclass BookList(Resource):\n    def get(self):\n        return books\n\n    def post(self):\n        parser = reqparse.RequestParser()\n        parser.add_argument(\"title\", type=str, required=True)\n        parser.add_argument(\"author\", type=str, required=True)\n        args = parser.parse_args()\n        new_book = {\n            \"id\": len(books) + 1,\n            \"title\": args[\"title\"],\n            \"author\": args[\"author\"]\n        }\n        books.append(new_book)\n        return new_book, 201\n\nclass Book(Resource):\n    def get(self, book_id):\n        book = next((book for book in books if book[\"id\"] == book_id), None)\n        if book is None:\n            return {\"error\": \"Book not found\"}, 404\n        return book\n\n    def put(self, book_id):\n        book = next((book for book in books if book[\"id\"] == book_id), None)\n        if book is None:\n            return {\"error\": \"Book not found\"}, 404\n        parser = reqparse.RequestParser()\n        parser.add_argument(\"title\", type=str)\n        parser.add_argument(\"author\", type=str)\n        args = parser.parse_args()\n        book[\"title\"] = args.get(\"title\", book[\"title\"])\n        book[\"author\"] = args.get(\"author\", book[\"author\"])\n        return book\n\n    def delete(self, book_id):\n        book = next((book for book in books if book[\"id\"] == book_id), None)\n        if book is None:\n            return {\"error\": \"Book not found\"}, 404\n        books.remove(book)\n        return {\"message\": \"Book deleted\"}\n\napi.add_resource(BookList, \"/books\")\napi.add_resource(Book, \"/books/<int:book_id>\")\n```\n\n---\n\n## üìä Endpoint Summary\n\n| Class | Method | Endpoint | Description |\n|-------|--------|----------|-------------|\n| BookList | GET | `/books` | Retrieve all books |\n| BookList | POST | `/books` | Create a new book |\n| Book | GET | `/books/<book_id>` | Retrieve a single book |\n| Book | PUT | `/books/<book_id>` | Update a book |\n| Book | DELETE | `/books/<book_id>` | Delete a book |\n\n---\n\n## ‚ñ∂Ô∏è Step 4: Run Your API\n\nRun your API using:\n\n```bash\npython app.py\n```\n\nYou can now interact with your API using tools like **curl** or a REST client like **Postman**.\n\n---\n\n## üìå Example Use Cases\n\n### 1. üìö Get all books\n\n```bash\ncurl http://localhost:5000/books\n```\n\n### 2. ‚ûï Create a new book\n\n```bash\ncurl -X POST -H \"Content-Type: application/json\" \\\n  -d '{\"title\": \"New Book\", \"author\": \"New Author\"}' \\\n  http://localhost:5000/books\n```\n\n### 3. üîç Get a single book\n\n```bash\ncurl http://localhost:5000/books/1\n```\n\n### 4. ‚úèÔ∏è Update a book\n\n```bash\ncurl -X PUT -H \"Content-Type: application/json\" \\\n  -d '{\"title\": \"Updated Book\"}' \\\n  http://localhost:5000/books/1\n```\n\n### 5. üóëÔ∏è Delete a book\n\n```bash\ncurl -X DELETE http://localhost:5000/books/1\n```\n\n---\n\n## üéØ Conclusion\n\nThis tutorial provides a basic setup for a Flask REST API. You can build upon this example to create more complex APIs with additional features like:\n\n- üîê **Authentication & Authorization** - JWT tokens, OAuth\n- üóÑÔ∏è **Database Integration** - SQLAlchemy, MongoDB\n- ‚úÖ **Input Validation** - More robust validation rules\n- üìù **API Documentation** - Swagger/OpenAPI\n- üß™ **Testing** - Unit tests and integration tests\n- üöÄ **Deployment** - Docker, Heroku, AWS\n\n**Happy coding!** üéâ",
    "summary": "A forward-looking piece on the ethical challenges of advanced AI and biotechnology in the year 2050.",
    "read_time": "8 min read",
    "tags": "Flask, REST API",
    "category": "Programming",
    "created_on": "2025-04-12 13:12:06",
    "updated_on": "2025-07-04 12:20:25",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "12",
    "slug": "python-regex",
    "title": "Regular Expressions for Python",
    "subtitle": "Harnessing the Power of Regex",
    "author": "Keith Thomson",
    "content": "# üîç Mastering Regular Expressions: A Comprehensive Guide\n\n## üìå Introduction\n\nRegular expressions (regex) are a **üî• powerful tool** for **pattern matching** and **text manipulation**. They allow you to **üîç search, üìù extract, and üîÑ replace** specific patterns within strings, making them invaluable for tasks like:\n- **‚úÖ Data validation**\n- **üìä Parsing**\n- **üîé Text mining**\n- **üìÇ Log analysis**\n- **üîÑ Search-and-replace operations**\n\nThis guide will introduce you to the **fundamental concepts, syntax, and real-world applications** of regular expressions.\n\n---\n\n## üìã Table of Contents\n1. [Basic Syntax](#basic-syntax)\n2. [Special Characters](#special-characters)\n3. [Grouping and Capturing](#grouping-and-capturing)\n4. [Lookaheads and Lookbehinds](#lookaheads-and-lookbehinds)\n5. [Common Use Cases](#common-use-cases)\n6. [Regex in Python](#regex-in-python)\n7. [Performance Considerations](#performance-considerations)\n8. [Practical Examples](#practical-examples)\n9. [Debugging and Testing](#debugging-and-testing)\n10. [Conclusion](#conclusion)\n\n---\n\n## üìñ Basic Syntax \n\n### üî§ Literal Characters\nMatch **exact characters**. For example, the regex `hello` will match the string `\"hello\"`.\n\n### üÖ∞Ô∏è Character Classes\nMatch **sets of characters**:\n   Syntax       | Description                                      | Example                     |\n |--------------|--------------------------------------------------|-----------------------------|\n | `[a-z]`      | Matches any lowercase letter.                    | `a`, `b`, `z`               |\n | `[A-Z]`      | Matches any uppercase letter.                    | `A`, `B`, `Z`               |\n | `[0-9]`      | Matches any digit.                               | `0`, `1`, `9`               |\n | `[a-zA-Z0-9]`| Matches any alphanumeric character.              | `a`, `B`, `1`               |\n | `[^a-z]`     | Matches any character **except** lowercase letters. | `A`, `1`, `@`           |\n\n---\n\n### üè∑Ô∏è Anchors\nMatch the **beginning or end** of a string:\n | Syntax | Description                                      | Example                     |\n |--------|--------------------------------------------------|-----------------------------|\n | `^`    | Matches the **beginning** of the string.         | `^hello` matches `\"hello world\"` |\n | `$`    | Matches the **end** of the string.               | `world$` matches `\"hello world\"` |\n\n---\n\n### üî¢ Quantifiers\nSpecify **how many times** a character or group should be repeated:\n | Syntax  | Description                                      | Example                     |\n |---------|--------------------------------------------------|-----------------------------|\n | `*`     | Matches **zero or more** occurrences.             | `a*` matches `\"\"`, `\"a\"`, `\"aa\"` |\n | `+`     | Matches **one or more** occurrences.              | `a+` matches `\"a\"`, `\"aa\"`  |\n | `?`     | Matches **zero or one** occurrence.               | `a?` matches `\"\"`, `\"a\"`    |\n | `{n}`   | Matches **exactly n** occurrences.                | `a{3}` matches `\"aaa\"`      |\n | `{n,}`  | Matches **n or more** occurrences.                | `a{2,}` matches `\"aa\"`, `\"aaa\"` |\n | `{n,m}` | Matches **between n and m** occurrences.          | `a{2,4}` matches `\"aa\"`, `\"aaa\"`, `\"aaaa\"` |\n\n---\n\n## ‚ö° Special Characters \n | Syntax | Description                                      | Example                     |\n |--------|--------------------------------------------------|-----------------------------|\n | `.`    | Matches **any character** (except newline).      | `a.c` matches `\"abc\"`, `\"a1c\"` |\n | `\\d`   | Matches a **digit**.                             | `\\d` matches `1`, `2`       |\n | `\\w`   | Matches a **word character**.                    | `\\w` matches `a`, `_`, `1`  |\n | `\\s`   | Matches **whitespace**.                          | `\\s` matches `\" \"`, `\\t`    |\n | `\\D`   | Matches a **non-digit** character.               | `\\D` matches `a`, `@`       |\n | `\\W`   | Matches a **non-word** character.                | `\\W` matches `@`, `#`       |\n | `\\S`   | Matches a **non-whitespace** character.          | `\\S` matches `a`, `1`       |\n\n---\n\n## ü§ù Grouping and Capturing \n\nParentheses `()` are used to **group** parts of a regex and **capture** matched text for extraction or backreferencing.\n | Syntax       | Description                                      | Example                     |\n |--------------|--------------------------------------------------|-----------------------------|\n | `(pattern)`  | Groups the pattern.                              | `(abc)`                     |\n | `\\1`, `\\2`   | Refer to captured groups (**backreferences**).   | `(a).\\1` matches `\"aba\"`    |\n\n**Example:**\nTo extract the **area code** and **phone number** from a string like `\"(123) 456-7890\"`:\n\n```regex\n$(\\d{3})$ (\\d{3}-\\d{4})\nPython Example:\nimport re\n\ntext = \"(123) 456-7890\"\npattern = r\"$(\\d{3})$ (\\d{3}-\\d{4})\"\nmatch = re.search(pattern, text)\n\nif match:\n    area_code = match.group(1)  # \"123\"\n    phone_number = match.group(2)  # \"456-7890\"\n    print(f\"üìû Area Code: {area_code}, Phone: {phone_number}\")\n```\n\n## üí° Common Use Cases \n| Type | Syntax |\n|------|--------|\n|‚úâÔ∏è Email Validation | ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\$ |\n| üåê Extracting URLs | https?://[^\\s]+ |\n| üìÖ Finding Dates | \\d{2}-\\d{2}-\\d{4} |\n| üîí Password Strength Check | ^(?=.*[A-Z])(?=.*[a-z])(?=.*\\d)(?=.*[@\\$!%*?&])[A-Za-z\\d@$!%*?&]{8,}$ |\n---\n* Validates email addresses (e.g., \"user@example.com\").\n* Matches HTTP/HTTPS URLs in text.\n* Matches dates in DD-MM-YYYY format.\n* Ensures passwords have at least one uppercase letter, one lowercase letter, one digit, one special character, and are at least 8 characters long.\n\n\n## üêç **Regex in Python** \nPython‚Äôs **re** module provides full support for regular expressions:\nimport re\n\n## üîç Search for a pattern\n```python\ntext = \"The quick brown fox jumps over the lazy dog.\"\nmatch = re.search(r\"brown \\w+\", text)\nprint(match.group())  # \"brown fox\"\n```\n\n## üìã Find all occurrences\n```python\nmatches = re.findall(r\"\\b\\w{3}\\b\", text)\nprint(matches)  # ['The', 'fox', 'the', 'dog']\n```\n\n### üîÑ Replace Text\n```python\nnew_text = re.sub(r\"fox\", \"cat\", text)\nprint(new_text)  # \"The quick brown cat jumps over the lazy dog.\"\n```\n\n---\n\n## ‚ö° Performance Considerations\n\n- ‚ö†Ô∏è Avoid greedy quantifiers (e.g., `.*`) when possible. Use non-greedy quantifiers (e.g., `.*?`) for efficiency.\n- üöÄ Pre-compile regex patterns for repeated use:\n  ```python\n  pattern = re.compile(r\"\\d{3}-\\d{4}\")\n  ```\n- Use specific patterns instead of generic ones (e.g., `\\d` instead of `.`).\n\n---\n\n## üìÇ Practical Examples {#Practical Examples} \n\n### 1. üè∑Ô∏è Extracting Hashtags\n```python\ntext = \"Love #regex! It's #awesome for #text processing.\"\nhashtags = re.findall(r\"#\\w+\", text)\nprint(hashtags)  # ['#regex', '#awesome', '#text']\n```\n\n### 2. üìú Parsing Log Files\n```python\nlog_entry = '127.0.0.1 - james [01/Jan/2025:12:34:56 +0000] \"GET /index.html\" 200 1234'\npattern = r'(\\S+) - (\\S+)$$\n(.*?)\n$$ \"(\\S+ \\S+)\" (\\d+) (\\d+)'\nmatch = re.search(pattern, log_entry)\nif match:\n    ip, user, date, request, status, size = match.groups()\n    print(f\"üñ•Ô∏è IP: {ip}, üë§ User: {user}, üìÑ Request: {request}\")\n```\n\n### 3. üìû Validating Phone Numbers\n```python\nphone_pattern = r'^(\\+\\d{1,3}[- ]?)?\\d{10}\\$'\nprint(re.match(phone_pattern, \"+1-1234567890\"))  # ‚úÖ Valid\nprint(re.match(phone_pattern, \"12345\"))  # ‚ùå Invalid\n```\n\n---\n\n## üõ†Ô∏è Debugging and Testing \n- Use online tools like [Regex101](https://regex101.com/) to test and debug regex patterns.\n- Break complex patterns into smaller, manageable parts.\n\n---\n\n## üìä Regex Cheat Sheet\n![Regex Cheat Sheet](https://i.imgur.com/OQStwMn.png)\nCredit: *https://i.imgur.com/OQStwMn.png*\n\n---\n\n\n## üéØ Conclusion \n\nRegular expressions are a versatile and powerful tool for text processing. By mastering the syntax and applying best practices, you can efficiently solve a wide range of string manipulation tasks. Start with simple patterns, gradually build complexity, and always test your regex against real-world data.\n\n**Next Steps:**\n- Practice with real-world datasets.\n- Explore regex in other programming languages (e.g., JavaScript, Perl).\n- Learn advanced techniques like recursive patterns and conditional matching.\n- Learn advanced techniques like recursive patterns and conditional matching.",
    "summary": "Step-by-step guide to building and deploying a REST API using Flask, covering routing, setup, and testing.",
    "read_time": "4 min read",
    "tags": "regex,regular expressions,pattern matching,TEXT processing,python,find and replace",
    "category": "Python",
    "created_on": "2025-04-13 01:30:33",
    "updated_on": "2025-07-11 12:44:22",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "13",
    "slug": "iot-sensors-future",
    "title": "IoT Sensors ",
    "subtitle": "The Future of Intelligent Sensing",
    "author": "Keith Thomson",
    "content": "# üì° The Rise of Intelligent Sensors: Powering the Next Wave of IoT\n\n## üåç Introduction\n\nBy **2030**, it's projected that there will be over **100 billion connected devices** around the globe. Behind every smart device lies one essential technology: **sensors**.\n\n**Examples of IoT devices:**\n- üå°Ô∏è Smart thermostats\n- ‚åö Wearable health trackers\n- üöÅ Autonomous drones\n- üè† Home automation systems\n\nAs the **core enablers of the Internet of Things (IoT)**, sensors are evolving from **basic signal collectors** into **intelligent, edge-processing nodes** that can understand and act on the world around them.\n\n---\n\n## üìà The Evolution of IoT Sensors\n\nEarlier generations of sensors simply collected **analog or digital data** (e.g., temperature, motion, voltage) and relayed it to a central server. Today's IoT sensors are **smarter**, thanks to:\n\n| Feature | Description |\n|---------|-------------|\n| üñ•Ô∏è **Microcontrollers** | Embedded directly within the sensor for local processing |\n| ü§ñ **On-device AI** | Models that interpret data at the edge, reducing cloud dependency |\n| ‚ö° **Power-efficient protocols** | BLE, LoRa, Zigbee for low-energy communication |\n| üîã **Energy harvesting** | Solar, thermal, or vibration-based power to extend battery life |\n\nThis evolution turns sensors into **autonomous decision-makers**, not just data providers.\n\n---\n\n## üå± Real-World Use Case: Agriculture\n\nIn **smart farming**, soil sensors measure **moisture, salinity, and temperature** to optimize irrigation schedules. \n\n### Modern sensor capabilities:\n- ‚úÖ Apply **threshold logic** or **anomaly detection** on-site\n- üîî Trigger **actuators or alerts** without human input\n- üìä Analyze patterns to predict crop health\n- üíß Optimize water usage automatically\n\n### Example: DHT22 Temperature & Humidity Sensor on Raspberry Pi\n\n```python\nimport Adafruit_DHT\n\nsensor = Adafruit_DHT.DHT22\npin = 4\n\nhumidity, temp = Adafruit_DHT.read_retry(sensor, pin)\nif humidity is not None and temp is not None:\n    print(f\"üå°Ô∏è Temperature: {temp:.1f}¬∞C | üíß Humidity: {humidity:.1f}%\")\nelse:\n    print(\"‚ùå Sensor read error\")\n```\n\n---\n\n## üèôÔ∏è Applications Across Industries\n\n### 1. üåÜ **Smart Cities**\n- üå¨Ô∏è Air quality monitoring\n- üí° Smart streetlights that adjust brightness\n- üö¶ Traffic sensors for congestion management\n- üöÆ Smart waste management systems\n\n### 2. üè• **Healthcare**\n- ‚åö Wearables that detect heart rate variability\n- üò∞ Stress level monitoring\n- üíä Medication adherence tracking\n- üöë Emergency alert systems\n\n### 3. üè≠ **Industrial IoT**\n- üì≥ Sensors monitor machinery vibrations\n- üîß Predict equipment failures before they happen\n- ‚ö° Energy consumption optimization\n- ü§ñ Automated quality control\n\n### 4. üå≥ **Environmental Monitoring**\n- üî• Forest fire early detection using heat and smoke sensors\n- üåä Flood prediction systems\n- üå§Ô∏è Weather monitoring stations\n- ü¶Ö Wildlife tracking and conservation\n\n---\n\n## ‚ö†Ô∏è Challenges Ahead\n\n### 1. üîó **Interoperability**\n- Many vendors, few unified standards\n- Difficulty integrating different systems\n- Need for common protocols\n\n### 2. üîí **Security**\n- Edge devices can be attack vectors\n- Physical access vulnerabilities\n- Data encryption requirements\n- Firmware update challenges\n\n### 3. üëÅÔ∏è **Data Privacy**\n- Always-on sensors raise surveillance concerns\n- GDPR and data protection compliance\n- User consent and transparency\n- Data retention policies\n\n---\n\n## üîÆ Conclusion\n\nThe future of IoT sensors lies not just in miniaturization, but in **autonomy**. With edge AI and smarter hardware, the next wave of sensors won't just measure the world‚Äîthey'll **respond to it**.\n\n### üåü The Impact:\n\n- üèôÔ∏è **Greener cities** - Reduced energy consumption\n- üè≠ **Efficient factories** - Predictive maintenance\n- üè† **Intuitive homes** - Seamless automation\n- üåç **Connected future** - Global IoT ecosystem\n\nSmart sensors are laying the foundation for a truly connected future where technology adapts to human needs seamlessly.\n\n**The sensor revolution is just beginning!** üöÄ",
    "summary": "An exploration of next-gen IoT sensors, their architecture, and how they‚Äôre shaping intelligent sensing systems.",
    "read_time": "5 min read",
    "tags": "rasberry pi,sensors,iot,edge computing,smart devices,data processing,data warehousing,sql,NoSQL",
    "category": "Python",
    "created_on": "2025-04-13 01:30:33",
    "updated_on": "2025-07-11 12:44:22",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "14",
    "slug": "using-finance-apis",
    "title": "Using Finance APIs to Build Smart Financial Tools",
    "subtitle": "Using Finance APIs to Build Smart Financial Tools",
    "author": "Keith Thomson",
    "content": "# üí∞ Using Finance APIs to Build Smart Financial Tools\n\n## üìä Introduction\n\nIn the age of real-time analytics, **Finance APIs** have become the cornerstone of modern financial applications. Whether you're building a stock tracking dashboard, a personal budgeting app, or an algorithmic trading bot, these APIs offer developers an open window into financial markets.\n\n**With just a few lines of code, you can access:**\n\n- üìà Live stock prices\n- üìâ Historical price charts\n- üíº Company financial statements\n- üåç Macroeconomic indicators\n\nThis guide explores how Finance APIs work, how to integrate them, and what to consider when building secure, scalable financial applications.\n\n---\n\n## üó∫Ô∏è Understanding the Landscape\n\nPopular Finance APIs include:\n\n### Leading Providers:\n\n1. **Alpha Vantage** - Best for free historical data and technical indicators\n2. **Finnhub** - Great for global markets and alternative datasets like news sentiment\n3. **IEX Cloud** - US-focused, reliable for intraday data\n4. **Yahoo Finance** (via RapidAPI) - Wide coverage but often requires a third-party wrapper\n\n### üì¶ What They Offer:\n\n- **üìä Equity Prices** - Real-time and historical stock data\n- **üìã Financial Statements** - Balance sheets, income statements, cash flow\n- **üí± Forex & Crypto** - Currency exchange rates and cryptocurrency prices\n- **üìà Economic Indicators** - GDP, CPI, interest rates, and more\n\n> üí° **Pro Tip:** Many offer a free tier suitable for prototyping, but watch out for rate limits (calls per minute or daily quotas).\n\n---\n\n## üéØ Choosing the Right API\n\n### Key Considerations:\n\n‚úÖ **Update Frequency**\n- Real-time vs. delayed data\n- How fresh do you need the data to be?\n\n‚úÖ **API Stability**\n- Response time consistency\n- Uptime and reliability\n\n‚úÖ **Pricing & Limits**\n- Free tier restrictions\n- Cost per API call\n\n‚úÖ **Licensing**\n- Commercial use restrictions\n- Data redistribution policies\n\n---\n\n## üíª Sample Integration: Alpha Vantage in Python\n\nHere's a basic example of how to retrieve and print live stock prices:\n\n```python\nimport requests\n\nAPI_KEY = \"your_api_key\"\nsymbol = \"AAPL\"\n\nurl = f\"https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol={symbol}&apikey={API_KEY}\"\nresponse = requests.get(url)\ndata = response.json()\n\nprice = data[\"Global Quote\"][\"05. price\"]\nvolume = data[\"Global Quote\"][\"06. volume\"]\nprint(f\"{symbol} Price: ${price} | Volume: {volume}\")\n```\n\n---\n\n## üõ†Ô∏è Building Smarter Tools\n\nWith real-time financial data, you can create:\n\n### üé® Project Ideas:\n\n1. **üö® Alert Systems**\n   - Set price thresholds\n   - Get notified when stocks hit targets\n\n2. **üìä Interactive Charts**\n   - Display moving averages\n   - Calculate RSI (Relative Strength Index)\n   - Visualize trends with candlestick charts\n\n3. **üíº Portfolio Trackers**\n   - Monitor holdings in real-time\n   - Calculate gains/losses\n   - Track diversification\n\n4. **ü§ñ Trading Bots** ‚ö†Ô∏è\n   - Algorithmic trading (carefully regulated)\n   - Backtesting strategies\n   - Paper trading for practice\n\n### ‚ö° Performance Tips:\n\n- **Cache data** to reduce API calls\n- **Debounce** frequent requests\n- Design UI with **delay tolerance** for free tier users\n- Use **webhooks** where available instead of polling\n\n---\n\n## üîí Security and Privacy\n\n### Essential Security Practices:\n\nüîê **API Key Management**\n- Store keys in environment variables\n- Never commit keys to version control\n- Rotate keys regularly\n\n‚úÖ **Data Validation**\n- Validate all API responses\n- Handle exceptions gracefully\n- Implement rate limiting on your end\n\nüõ°Ô∏è **User Data Protection**\n- Encrypt portfolio data at rest\n- Use proper authentication/authorization\n- Comply with financial data regulations\n- Never expose sensitive user information\n\n---\n\n## üéØ Conclusion\n\nFinance APIs unlock **powerful capabilities** for developers at any level:\n\n- üéì **Hobbyists** - Build learning projects and personal dashboards\n- üíº **Professionals** - Create production-grade fintech platforms\n- üöÄ **Startups** - Launch innovative financial products\n\n**The key is understanding your data, handling it responsibly, and building for performance and scale.**\n\n### üìö Next Steps:\n\n1. Sign up for a free API key\n2. Build a simple stock price tracker\n3. Add visualizations with charts\n4. Implement caching and error handling\n5. Scale up to real-time applications\n\n**Remember:** Finance APIs bridge the gap between global markets and modern software. Use them wisely! üåü",
    "summary": "Master regular expressions in Python with practical examples, syntax breakdowns, and performance tips.",
    "read_time": "6 min read",
    "tags": "finance,api,stock market,real-time data,fintech",
    "category": "General",
    "created_on": "2025-04-13 01:30:33",
    "updated_on": "2025-07-11 12:44:22",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "15",
    "slug": "ethics-2050",
    "title": "Ethics for The Future of Humanity",
    "subtitle": "Big Data, Smart Homes, Surveillance and Tracking... ",
    "author": "Keith Thomson",
    "content": "# üîÆ Ethics for The Future of Humanity\n\n## üåê Introduction\n\nAs we approach **2050**, humanity stands at a crossroads shaped by exponential advancements in:\n\n- ü§ñ **Artificial Intelligence**\n- üß¨ **Biotechnology**\n- üëÅÔ∏è **Surveillance Infrastructure**\n- üèõÔ∏è **Digital Governance**\n\nEthical frameworks that guided the last century ‚Äî centered on **individual rights**, **informed consent**, and **moral agency** ‚Äî are increasingly strained by emerging technologies that challenge the very notion of what it means to be human.\n\nIn this article, we'll explore the **ethical frontiers of the future** and how technologists, lawmakers, and citizens might respond.\n\n---\n\n## ü§ñ AI Decision-Making and Responsibility\n\n### The Challenge\n\nBy 2050, many **life-affecting decisions** may be delegated to AI:\n\n- ‚öñÔ∏è Parole rulings and sentencing\n- üè• Medical triage and treatment recommendations\n- üè¶ Loan approvals and credit scoring\n- üéì University admissions\n- üíº Hiring and employment decisions\n\n**But who is accountable when an AI fails?**\n\n### üö® Key Ethical Concerns:\n\n1. **Algorithmic Bias**\n   - Decisions based on biased historical data\n   - Perpetuating systemic inequalities\n   - Discriminatory outcomes\n\n2. **Opacity of Black-Box Models**\n   - Inability to explain AI decisions\n   - Loss of transparency\n   - Erosion of trust\n\n3. **Lack of Appeal Mechanisms**\n   - No recourse for algorithmic decisions\n   - Difficulty challenging automated outcomes\n   - Removal of human oversight\n\n### üíª Pseudo-Code: Ethics Check in AI\n\n```python\ndef decide_action(data):\n    # Ensure ethical traceability\n    if not audit_trail(data):\n        raise Exception(\"No ethical traceability\")\n    \n    # Check for bias\n    if is_biased(data):\n        return \"Flag for review\"\n    \n    # Proceed with AI decision\n    return ai_model.predict(data)\n```\n\n---\n\n## üëÅÔ∏è Surveillance vs Consent\n\n### The Dilemma\n\nWith **ubiquitous surveillance** technologies, the line between safety and control becomes dangerously thin:\n\n- üì∏ Facial recognition everywhere\n- üö∂ Gait analysis tracking\n- üíì Bio-metric monitoring\n- üß† Emotional state detection\n\n### üî¥ Scenarios of 2050:\n\n1. **Social Credit Systems**\n   - Government-issued AR glasses displaying citizen \"reputation scores\"\n   - Real-time behavior tracking and rating\n   - Restricted access based on social standing\n\n2. **Workplace & Education Surveillance**\n   - Emotional detection cameras in classrooms\n   - Productivity monitoring in offices\n   - Constant biometric authentication\n\n3. **Predictive Policing**\n   - Pre-crime arrest systems based on behavioral models\n   - AI-predicted \"likelihood to offend\"\n   - Loss of presumption of innocence\n\n### ‚ö†Ô∏è What's at Stake:\n\n> Ethics demand a **redefinition of privacy, autonomy, and democratic oversight**. We must ask: At what point does safety become oppression?\n\n---\n\n## üß¨ Human Enhancement and Inequality\n\n### The Technology\n\nAdvanced enhancement technologies may include:\n\n- üß¨ **CRISPR-edited intelligence** - Genetic modifications for cognitive enhancement\n- üß† **Memory implants** - Digital storage and recall augmentation\n- üîó **Neuro-linked AI assistants** - Direct brain-computer interfaces\n- üí™ **Physical enhancements** - Strength, endurance, and longevity modifications\n\n### ü§î Critical Questions:\n\n1. **Access and Equity**\n   - Who gets access to enhancement?\n   - Will it be available only to the wealthy?\n   - How do we prevent a \"genetic divide\"?\n\n2. **Regulation vs Freedom**\n   - Should we regulate human potential?\n   - Where do we draw the line?\n   - Can enhancement be mandated or restricted?\n\n3. **Identity and Humanity**\n   - Is a post-human future inevitable?\n   - What defines \"human\" anymore?\n   - How do we preserve human dignity?\n\n### üåç Society Divided:\n\n```\nThe Modified                vs                The Natural\n‚îú‚îÄ Enhanced cognition                ‚îú‚îÄ Traditional humanity\n‚îú‚îÄ Extended lifespan                 ‚îú‚îÄ Unmodified genetics\n‚îú‚îÄ Superior abilities                ‚îú‚îÄ Organic limitations\n‚îî‚îÄ Premium access                    ‚îî‚îÄ Economic barriers\n```\n\n---\n\n## üèõÔ∏è Governance and Ethical AI\n\n### Emerging Frameworks\n\nBy 2050, nations and organizations may have:\n\n#### 1. üè¢ **AI Ethics Boards**\n- Embedded in tech companies\n- Independent oversight committees\n- Transparent decision-making processes\n- Public accountability mechanisms\n\n#### 2. üåê **Global AI Treaties**\n- Digital Geneva Conventions\n- International AI safety standards\n- Cross-border data protection agreements\n- Shared ethical principles\n\n#### 3. ‚öñÔ∏è **Robot Rights**\n- Should autonomous agents have legal status?\n- Personhood for advanced AI?\n- Rights and responsibilities of synthetic beings\n- Ethical treatment of conscious machines\n\n### üìã Proposed Principles:\n\n‚úÖ **Transparency** - Explainable AI decisions  \n‚úÖ **Accountability** - Clear chains of responsibility  \n‚úÖ **Fairness** - Bias mitigation and equity  \n‚úÖ **Privacy** - Data protection and consent  \n‚úÖ **Safety** - Robust testing and fail-safes  \n‚úÖ **Human Control** - Meaningful human oversight\n\n---\n\n## üéØ Conclusion\n\n### The Path Forward\n\nEthics in 2050 **isn't science fiction** ‚Äî it's the blueprint of how we shape:\n\n- üèõÔ∏è **Society** - How we live together\n- ü™™ **Identity** - Who we are and become\n- ‚öñÔ∏è **Justice** - What's fair and right\n\n**In the face of overwhelming technological change.**\n\n### üí° Key Takeaways:\n\n1. **Don't Reject Technology** - It's not about stopping progress\n2. **Embed Humanity** - Technology must serve human values\n3. **Engineer with Empathy** - Consider impact on all people\n4. **Demand Accountability** - Build oversight into systems\n5. **Plan with Foresight** - Anticipate consequences\n\n---\n\n### üåü Final Thought\n\n> *\"The future must be engineered with empathy, accountability, and foresight. The question isn't whether we can build it, but whether we should ‚Äî and if so, how we ensure it serves all of humanity, not just the privileged few.\"*\n\n**The ethical choices we make today will echo through the centuries to come.** üåç‚ú®",
    "summary": "A practical guide to using financial APIs to build smarter investment and analysis tools for developers and analysts.",
    "read_time": "7 min read",
    "tags": "surveillance,ethics,AI,biotechnology,digital governance,human enhancement,facial recognition,CRISPR,neuro-link",
    "category": "AI, Ethics, Biotechnology",
    "created_on": "2025-04-13 01:30:33",
    "updated_on": "2025-07-11 12:44:22",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "19",
    "slug": "getting-started-with-rust",
    "title": "Getting Started with Rust",
    "subtitle": "A comprehensive guide to learning Rust programming language",
    "author": "Keith Thomson",
    "content": "# üöÄ Getting Started with Rust: Why It‚Äôs Changing the Game\n\nRust is a **systems programming language** that runs blazingly fast, prevents segfaults, and guarantees thread safety. In this post, we‚Äôll explore the **fundamentals of Rust**, why it‚Äôs becoming increasingly popular among developers, and walk through practical examples that show how Rust differs from other languages.  \n\n\n\n## üåü Why Rust?  \n\nRust offers several advantages over traditional systems programming languages like **C** and **C++**:  \n\n- **Memory Safety**: Prevents common bugs like null pointer dereferences and buffer overflows.  \n- **Performance**: Zero-cost abstractions ‚Äî you don‚Äôt pay for features you don‚Äôt use.  \n- **Concurrency**: Built-in support for safe, data-race-free concurrent programming.  \n- **Modern Ecosystem**: A strong package manager (`cargo`), vibrant community, and modern tooling.  \n\n> üí° **Tip:** Rust enforces correctness at compile time, saving you from runtime surprises that are common in C/C++.  \n\n---\n\n## üî§ Basic Concepts  \n\nLet‚Äôs start with the classic **Hello, World!** program:  \n\n```rust\nfn main() {\n    println!(\"Hello, World!\");\n}\n```\n\nThis simple program demonstrates Rust‚Äôs **clean syntax** and its **macro system** (notice the `!` in `println!`). Macros in Rust are more powerful than standard functions ‚Äî they can generate code at compile time.  \n\n---\n\n## üìù Variables and Mutability  \n\nIn Rust, variables are **immutable by default**. This means once you assign a value, it cannot change unless you explicitly declare it as mutable.  \n\n```rust\nfn main() {\n    let x = 5;        // immutable\n    let mut y = 10;   // mutable\n\n    println!(\"x = {}\", x);\n    println!(\"y = {}\", y);\n\n    y = 15;\n    println!(\"y (after change) = {}\", y);\n}\n```\n\n- `let` creates a variable.  \n- `mut` makes it mutable.  \n- Rust encourages immutability to reduce bugs and improve safety.  \n\n---\n\n## üîë Ownership and Borrowing  \n\nRust‚Äôs **ownership system** is its most unique feature. It enforces memory safety without a garbage collector.  \n\n### Example: Ownership  \n\n```rust\nfn main() {\n    let s1 = String::from(\"Rust\");\n    let s2 = s1; // ownership moved from s1 to s2\n\n    // println!(\"{}\", s1); // ‚ùå Error: s1 is no longer valid\n    println!(\"{}\", s2);   // ‚úÖ Works\n}\n```\n\n- Variables own their data.  \n- When ownership is transferred (moved), the old variable is invalidated.  \n\n### Example: Borrowing  \n\n```rust\nfn main() {\n    let s = String::from(\"Borrowing in Rust\");\n    print_length(&s); // pass reference (borrow)\n    println!(\"s is still valid: {}\", s);\n}\n\nfn print_length(s: &String) {\n    println!(\"Length: {}\", s.len());\n}\n```\n\n- `&` means ‚Äúborrow without taking ownership.‚Äù  \n- The original variable remains valid.  \n\n> üîí This system prevents dangling pointers and memory leaks at compile time.  \n\n---\n\n## üîß Functions and Control Flow  \n\nRust functions look familiar, but with strong typing and return value rules.  \n\n```rust\nfn main() {\n    println!(\"Sum = {}\", add(5, 7));\n\n    let number = 6;\n    if number % 2 == 0 {\n        println!(\"Even\");\n    } else {\n        println!(\"Odd\");\n    }\n}\n\nfn add(a: i32, b: i32) -> i32 {\n    a + b  // no semicolon = return value\n}\n```\n\n- Functions must declare parameter and return types.  \n- Leaving out the semicolon `;` makes it an **expression** that returns a value.  \n\n---\n\n## ‚ö†Ô∏è Error Handling  \n\nRust does not have exceptions. Instead, it uses:  \n- `Result<T, E>` for recoverable errors.  \n- `Option<T>` for values that may or may not exist.  \n\n```rust\nuse std::fs::File;\nuse std::io::ErrorKind;\n\nfn main() {\n    let file = File::open(\"data.txt\");\n\n    match file {\n        Ok(_) => println!(\"File opened successfully.\"),\n        Err(ref e) if e.kind() == ErrorKind::NotFound => {\n            println!(\"File not found, creating one...\");\n        }\n        Err(e) => {\n            println!(\"Error: {:?}\", e);\n        }\n    }\n}\n```\n\nThis forces you to **handle errors explicitly**.  \n\n---\n\n## üßµ Concurrency  \n\nRust makes concurrency safe by design. Threads must follow ownership and borrowing rules.  \n\n```rust\nuse std::thread;\n\nfn main() {\n    let handles: Vec<_> = (1..5).map(|i| {\n        thread::spawn(move || {\n            println!(\"Hello from thread {}\", i);\n        })\n    }).collect();\n\n    for handle in handles {\n        handle.join().unwrap();\n    }\n}\n```\n\n- `move` transfers ownership into the thread.  \n- No data races are possible because Rust enforces safe access at compile time.  \n\n---\n\n## üèÅ Conclusion  \n\nRust combines **the performance of C/C++** with **modern safety guarantees** and a thriving ecosystem. Its ownership model may take time to learn, but it pays off by eliminating entire classes of bugs.  \n\nWhether you‚Äôre building:  \n- üöÄ **High-performance applications**  \n- üåê **Web servers with frameworks like Actix or Axum**  \n- üìä **Data pipelines and concurrent systems**  \n- üîí **Secure, low-level embedded software**  \n\nRust is quickly becoming the **go-to language for systems programming** in the modern era.  \n\n---\n\nüí° *Next Step:* Try rewriting a small project you‚Äôve built in Python, Go, or C into Rust ‚Äî you‚Äôll immediately see how ownership, borrowing, and safety rules shape your design.  ",
    "summary": "Learn the basics of Rust programming language, from installation to writing your first program.",
    "read_time": "8 min read \n",
    "tags": "rust,programming,tutorial",
    "category": "Programming",
    "created_on": "2025-07-10 16:15:45",
    "updated_on": "2025-07-10 16:15:45",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "20",
    "slug": "building-web-apps-with-axum",
    "title": "Building Web Applications with Axum",
    "subtitle": "Modern web development using Rust and the Axum framework",
    "author": "Keith Thomson",
    "content": "# ‚ö° Building Web Applications with Axum in Rust  \n\nAxum is a **web application framework** that focuses on ergonomics and modularity. Built on top of **Tokio** (for async runtime) and **Tower** (for middleware and services), it provides a solid foundation for building **scalable, fast, and reliable web applications** in Rust.  \n\nIn this post, we‚Äôll walk through setting up an Axum project, creating routes, handling requests, adding middleware, and building APIs with JSON.  \n\n\n## üîß Setting Up Your Project  \n\nFirst, let‚Äôs create a new Rust project and add Axum as a dependency:  \n\n```bash\ncargo new my-web-app\ncd my-web-app\ncargo add axum tokio --features tokio/full\ncargo add tower-http --features full\ncargo add serde serde_json --features derive\n```  \n\nThis will set up a new project with **Axum**, **Tokio**, **Tower HTTP utilities**, and **Serde** for JSON support.  \n\n---\n\n## üåç Creating Your First Route  \n\nHere‚Äôs how to create a simple HTTP server with Axum:  \n\n```rust\nuse axum::{\n    routing::get,\n    Router,\n};\n\n#[tokio::main]\nasync fn main() {\n    let app = Router::new()\n        .route(\"/\", get(|| async { \"Hello, World!\" }));\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n```\n\nThis creates a basic web server that responds with `\"Hello, World!\"` on the root path.  \n\nRun it with:  \n\n```bash\ncargo run\n```  \n\nVisit [http://localhost:3000](http://localhost:3000) in your browser to see it in action.  \n\n---\n\n## üîó Handling Path and Query Parameters  \n\nAxum makes it easy to capture path and query parameters.  \n\n```rust\nuse axum::{extract::Path, routing::get, Router};\n\n#[tokio::main]\nasync fn main() {\n    let app = Router::new()\n        .route(\"/hello/:name\", get(greet));\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n\nasync fn greet(Path(name): Path<String>) -> String {\n    format!(\"Hello, {}!\", name)\n}\n```\n\nNow, visiting `/hello/Alice` will return:  \n\n```\nHello, Alice!\n```  \n\nYou can also extract query parameters using `axum::extract::Query`.  \n\n---\n\n## üì¶ Returning JSON Responses  \n\nAxum integrates with `serde` for JSON serialization.  \n\n```rust\nuse axum::{routing::get, Json, Router};\nuse serde::Serialize;\n\n#[derive(Serialize)]\nstruct Message {\n    message: String,\n}\n\n#[tokio::main]\nasync fn main() {\n    let app = Router::new()\n        .route(\"/json\", get(get_message));\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n\nasync fn get_message() -> Json<Message> {\n    Json(Message {\n        message: \"Hello from JSON!\".to_string(),\n    })\n}\n```\n\nVisiting `/json` will return:  \n\n```json\n{\"message\": \"Hello from JSON!\"}\n```  \n\n---\n\n## üõ° Adding Middleware  \n\nAxum builds on **Tower**, so you can add middleware like logging, timeouts, or request limits.  \n\n```rust\nuse axum::{\n    routing::get,\n    Router,\n};\nuse tower_http::trace::TraceLayer;\n\n#[tokio::main]\nasync fn main() {\n    let app = Router::new()\n        .route(\"/\", get(|| async { \"Hello with middleware!\" }))\n        .layer(TraceLayer::new_for_http()); // log requests\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n```\n\nThis logs each request/response, useful for debugging and monitoring.  \n\n---\n\n## üî® Building a Small REST API  \n\nHere‚Äôs a simple **in-memory todo API** with Axum:  \n\n```rust\nuse axum::{\n    extract::{Path, State},\n    routing::{get, post},\n    Json, Router,\n};\nuse serde::{Deserialize, Serialize};\nuse std::sync::{Arc, Mutex};\n\n#[derive(Serialize, Deserialize, Clone)]\nstruct Todo {\n    id: usize,\n    text: String,\n}\n\n#[derive(Clone, Default)]\nstruct AppState {\n    todos: Arc<Mutex<Vec<Todo>>>,\n}\n\n#[tokio::main]\nasync fn main() {\n    let state = AppState::default();\n\n    let app = Router::new()\n        .route(\"/todos\", get(list_todos).post(add_todo))\n        .route(\"/todos/:id\", get(get_todo))\n        .with_state(state);\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n\nasync fn list_todos(State(state): State<AppState>) -> Json<Vec<Todo>> {\n    let todos = state.todos.lock().unwrap().clone();\n    Json(todos)\n}\n\nasync fn add_todo(State(state): State<AppState>, Json(todo): Json<Todo>) -> Json<Todo> {\n    let mut todos = state.todos.lock().unwrap();\n    todos.push(todo.clone());\n    Json(todo)\n}\n\nasync fn get_todo(Path(id): Path<usize>, State(state): State<AppState>) -> Option<Json<Todo>> {\n    let todos = state.todos.lock().unwrap();\n    todos.iter().find(|t| t.id == id).cloned().map(Json)\n}\n```\n\nEndpoints:  \n- `GET /todos` ‚Üí List todos  \n- `POST /todos` ‚Üí Add a todo (JSON body)  \n- `GET /todos/:id` ‚Üí Fetch a todo by ID  \n\n---\n\n## üèÅ Conclusion  \n\nAxum provides:  \n- Clean, ergonomic APIs for routing and request handling.  \n- Native async support via Tokio.  \n- Integration with Tower for middleware.  \n- Strong type safety and Rust‚Äôs memory guarantees.  \n\nIt‚Äôs an excellent choice for building **web servers, REST APIs, and microservices** in Rust.  \n\nüí° *Next Step:* Extend this project with persistent storage (SQLite, Postgres, or Redis) to turn it into a production-ready API.  ",
    "summary": "Learn how to build modern web applications using Rust and the Axum framework.",
    "read_time": "12 min read",
    "tags": "rust,web,axum,backend",
    "category": "Web Development",
    "created_on": "2025-07-10 16:15:45",
    "updated_on": "2025-07-10 16:15:45",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "21",
    "slug": "async-rust-programming",
    "title": "Mastering Async Programming in Rust",
    "subtitle": "Understanding futures, async/await, and concurrent programming patterns",
    "author": "Keith Thomson",
    "content": "# ‚è≥ Mastering Asynchronous Programming in Rust  \n\nAsynchronous programming is one of **Rust's most powerful features**, enabling you to write **highly concurrent and performant applications**. This guide will walk you through the fundamentals of async Rust, covering futures, the async/await syntax, and practical examples using the Tokio runtime.  \n\n\n## üîÆ Understanding Futures  \n\nIn Rust, asynchronous operations are represented by **futures**. A future is a value that may not be available yet, but will be at some point in the future.  \n\nA future does nothing on its own until it is **polled** by an executor (like Tokio).  \n\n```rust\nuse std::future::Future;\n\nfn example_future() -> impl Future<Output = i32> {\n    async {\n        42\n    }\n}\n```\n\nHere, the future resolves to `42` once awaited.  \n\n---\n\n## üìù The async/await Syntax  \n\nThe `async` keyword turns a function into a future, and `await` is used to wait for that future to complete:  \n\n```rust\nuse tokio::time::{sleep, Duration};\n\nasync fn fetch_data() -> String {\n    // Simulate async work\n    sleep(Duration::from_secs(1)).await;\n    \"Data fetched!\".to_string()\n}\n\n#[tokio::main]\nasync fn main() {\n    let result = fetch_data().await;\n    println!(\"{}\", result);\n}\n```\n\nThis **non-blocking approach** allows your application to handle thousands of concurrent operations efficiently.  \n\n---\n\n## üßµ Spawning Tasks with Tokio  \n\nTokio provides an async runtime for executing tasks concurrently. You can spawn lightweight async tasks using `tokio::spawn`:  \n\n```rust\nuse tokio::time::{sleep, Duration};\n\nasync fn task(id: i32) {\n    println!(\"Task {} started\", id);\n    sleep(Duration::from_secs(2)).await;\n    println!(\"Task {} finished\", id);\n}\n\n#[tokio::main]\nasync fn main() {\n    let handle1 = tokio::spawn(task(1));\n    let handle2 = tokio::spawn(task(2));\n\n    // Wait for both tasks to finish\n    let _ = tokio::join!(handle1, handle2);\n}\n```\n\nOutput (order may vary):  \n```\nTask 1 started\nTask 2 started\nTask 1 finished\nTask 2 finished\n```\n\n---\n\n## üìÇ Using async with I/O  \n\nAsync is especially powerful when handling **I/O-bound tasks** like networking or file access.  \n\nExample: Reading from TCP using async:  \n\n```rust\nuse tokio::net::TcpListener;\nuse tokio::io::{AsyncReadExt, AsyncWriteExt};\n\n#[tokio::main]\nasync fn main() -> tokio::io::Result<()> {\n    let listener = TcpListener::bind(\"127.0.0.1:8080\").await?;\n\n    loop {\n        let (mut socket, _) = listener.accept().await?;\n\n        tokio::spawn(async move {\n            let mut buf = [0; 1024];\n            let n = socket.read(&mut buf).await.unwrap();\n\n            if n > 0 {\n                socket.write_all(&buf[0..n]).await.unwrap();\n            }\n        });\n    }\n}\n```\n\nThis creates a simple **async echo server**.  \n\n---\n\n## ‚ö†Ô∏è Error Handling in Async  \n\nYou can use `Result` and the `?` operator with async functions just like synchronous code:  \n\n```rust\nuse tokio::fs::File;\nuse tokio::io::{self, AsyncReadExt};\n\nasync fn read_file(path: &str) -> io::Result<String> {\n    let mut file = File::open(path).await?;\n    let mut contents = String::new();\n    file.read_to_string(&mut contents).await?;\n    Ok(contents)\n}\n\n#[tokio::main]\nasync fn main() -> io::Result<()> {\n    match read_file(\"example.txt\").await {\n        Ok(data) => println!(\"File contents: {}\", data),\n        Err(e) => println!(\"Error: {}\", e),\n    }\n    Ok(())\n}\n```\n\n---\n\n## üî® Building a Simple Async App  \n\nLet‚Äôs combine everything into a **mini async downloader**:  \n\n```rust\nuse reqwest;\nuse tokio;\n\nasync fn fetch_url(url: &str) -> reqwest::Result<String> {\n    let response = reqwest::get(url).await?;\n    let body = response.text().await?;\n    Ok(body)\n}\n\n#[tokio::main]\nasync fn main() {\n    let url = \"https://www.rust-lang.org\";\n    match fetch_url(url).await {\n        Ok(html) => println!(\"Downloaded {} bytes\", html.len()),\n        Err(e) => println!(\"Error: {}\", e),\n    }\n}\n```\n\nThis example fetches the HTML of Rust‚Äôs official site asynchronously.  \n\n---\n\n## üèÅ Conclusion  \n\nAsync Rust lets you:  \n- Handle **thousands of concurrent tasks** efficiently.  \n- Write **non-blocking I/O operations** with Tokio.  \n- Use familiar patterns like `async/await`, `Result`, and `?`.  \n- Build **high-performance servers** and **networked apps**.  \n\nüí° *Next Step:* Try combining Axum and async Rust to build a full-featured web API ‚Äî you‚Äôll see the true power of Rust‚Äôs async ecosystem.  ",
    "summary": "Master asynchronous programming in Rust with futures, async/await, and concurrent patterns.",
    "read_time": "15 min read \n",
    "tags": "rust,async,concurrency,programming",
    "category": "Programming",
    "created_on": "2025-07-10 16:15:45",
    "updated_on": "2025-07-10 16:15:45",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "22",
    "slug": "go-data-structures",
    "title": "Data Structures in Go: A Comprehensive Guide",
    "subtitle": "Exploring arrays, slices, maps, structs, and more in Go",
    "author": "Keith Thomson",
    "content": "# üêπ Data Structures in Go: A Comprehensive Guide  \n\n\n#### **Go** (Golang) provides a rich set of built-in and library-> supported data structures that make it powerful for both systems programming and application development. \n\nIn this guide, we‚Äôll explore the core data structures available in Go, explain how they work, and show practical code examples.  \n\n\n## üî¢ Arrays  \n\nAn **array** in Go is a fixed-size, ordered collection of elements.  \n\n```go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    var arr [3]int = [3]int{1, 2, 3}\n    fmt.Println(arr)\n\n    // Iterate\n    for i, v := range arr {\n        fmt.Printf(\"Index %d = %d\n\", i, v)\n    }\n}\n```\n\n- Arrays have a fixed length.  \n- Useful when the size is known and constant.  \n\n\n\n## üìê Slices  \n\nA **slice** is a dynamically-sized, flexible view into an array. Slices are the most commonly used data structure in Go.  \n\n```go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    slice := []int{1, 2, 3, 4, 5}\n    slice = append(slice, 6)\n    fmt.Println(slice)\n    fmt.Println(\"Length:\", len(slice), \"Capacity:\", cap(slice))\n}\n```\n\n- Built on top of arrays.  \n- Support dynamic resizing with `append`.  \n- Preferred over arrays in most cases.  \n\n\n\n## üó∫ Maps  \n\nA **map** is Go‚Äôs built-in hash table implementation for key-value pairs.  \n\n```go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    m := map[string]int{\n        \"Alice\": 25,\n        \"Bob\":   30,\n    }\n    m[\"Charlie\"] = 35\n\n    for k, v := range m {\n        fmt.Printf(\"%s is %d years old\n\", k, v)\n    }\n}\n```\n\n- Keys must be comparable (e.g., strings, ints).  \n- Lookups are O(1) on average.  \n\n\n\n## üèó Structs  \n\nA **struct** groups fields together, making it Go‚Äôs way of creating custom data types.  \n\n```go\npackage main\n\nimport \"fmt\"\n\ntype Person struct {\n    Name string\n    Age  int\n}\n\nfunc main() {\n    p := Person{Name: \"Alice\", Age: 30}\n    fmt.Println(p.Name, \"is\", p.Age)\n}\n```\n\n- Structs are used for modeling entities.  \n- They are the building blocks of more complex data structures.  \n\n\n## üîó Linked Lists  \n\nGo‚Äôs standard library provides a **doubly linked list** via `container/list`.  \n\n```go\npackage main\n\nimport (\n    \"container/list\"\n    \"fmt\"\n)\n\nfunc main() {\n    l := list.New()\n    l.PushBack(1)\n    l.PushBack(2)\n    l.PushFront(0)\n\n    for e := l.Front(); e != nil; e = e.Next() {\n        fmt.Println(e.Value)\n    }\n}\n```\n\n- Each element points to the next and previous nodes.  \n- Efficient for insertions/removals in the middle.  \n\n\n\n## üìö Stacks  \n\nA **stack** is a LIFO (Last In, First Out) structure. Implemented easily with slices.  \n\n```go\npackage main\n\nimport \"fmt\"\n\ntype Stack []int\n\nfunc (s *Stack) Push(v int) {\n    *s = append(*s, v)\n}\n\nfunc (s *Stack) Pop() int {\n    if len(*s) == 0 {\n        panic(\"stack is empty\")\n    }\n    val := (*s)[len(*s)-1]\n    *s = (*s)[:len(*s)-1]\n    return val\n}\n\nfunc main() {\n    var s Stack\n    s.Push(10)\n    s.Push(20)\n    fmt.Println(s.Pop()) // 20\n}\n```\n\n- Built using slices.  \n- Great for recursion-like problems, parsing, and backtracking.  \n\n---\n\n## üì¨ Queues  \n\nA **queue** is a FIFO (First In, First Out) structure. Also implemented with slices.  \n\n```go\npackage main\n\nimport \"fmt\"\n\ntype Queue []int\n\nfunc (q *Queue) Enqueue(v int) {\n    *q = append(*q, v)\n}\n\nfunc (q *Queue) Dequeue() int {\n    if len(*q) == 0 {\n        panic(\"queue is empty\")\n    }\n    val := (*q)[0]\n    *q = (*q)[1:]\n    return val\n}\n\nfunc main() {\n    var q Queue\n    q.Enqueue(1)\n    q.Enqueue(2)\n    fmt.Println(q.Dequeue()) // 1\n}\n```\n\n- Useful for scheduling and breadth-first search (BFS).  \n\n\n\n## ‚õ∞ Heaps & Priority Queues  \n\nGo provides heap operations in the `container/heap` package.  \n\n```go\npackage main\n\nimport (\n    \"container/heap\"\n    \"fmt\"\n)\n\ntype IntHeap []int\n\nfunc (h IntHeap) Len() int           { return len(h) }\nfunc (h IntHeap) Less(i, j int) bool { return h[i] < h[j] }\nfunc (h IntHeap) Swap(i, j int)      { h[i], h[j] = h[j], h[i] }\n\nfunc (h *IntHeap) Push(x any) {\n    *h = append(*h, x.(int))\n}\n\nfunc (h *IntHeap) Pop() any {\n    old := *h\n    n := len(old)\n    x := old[n-1]\n    *h = old[0 : n-1]\n    return x\n}\n\nfunc main() {\n    h := &IntHeap{3, 1, 4}\n    heap.Init(h)\n    heap.Push(h, 2)\n    fmt.Println(heap.Pop(h)) // 1 (smallest element)\n}\n```\n\n- Implements a min-heap by default.  \n- Can be adapted into a priority queue.  \n\n---\n\n## üèÅ Conclusion  \n\nGo provides both **high-level abstractions** (slices, maps, structs) and **low-level control** (linked lists, heaps).  \nBy mastering these data structures, you‚Äôll be ready to build efficient algorithms, design scalable applications, and handle both systems and business logic effectively.  \n\nüí° *Next Step:* Try implementing algorithms like BFS, DFS, and Dijkstra‚Äôs algorithm using these data structures to strengthen your understanding.",
    "summary": "A deep dive into Go‚Äôs built-in and library-supported data structures with examples.",
    "read_time": "15 min read",
    "tags": "golang,data structures,programming",
    "category": "Golang",
    "created_on": "2025-08-23 14:24:56",
    "updated_on": "2025-08-23 14:24:56",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "23",
    "slug": "google-big-query-python",
    "title": "Using Google BigQuery with Python: A Full Guide",
    "subtitle": "A Google BigQuery Tutorial ",
    "author": "Keith Thomson",
    "content": "# Using Google BigQuery with Python \n\n## A Practical Guide ü¶Æ\n\n![](https://locusit.com/wp-content/uploads/2024/12/Google-BigQuery.jpeg)\n\nGoogle BigQuery is a fully-managed, serverless data warehouse that enables scalable analysis over petabytes of data. When combined with Python üêç, it becomes a powerful tool for data engineers, analysts, and scientists.\n\nThis guide provides **real-world code examples** and best practices for integrating BigQuery with Python on Google Cloud Platform (GCP).\n\n---\n![](https://www.python.org/static/community_logos/python-logo-master-v3-TM.png)\n\n\n## Table of Contents\n1. [Prerequisites](#prerequisites)\n2. [Setting Up Authentication](#setting-up-authentication)\n3. [Connecting to BigQuery](#connecting-to-bigquery)\n4. [Querying Data from BigQuery](#querying-data-from-bigquery)\n5. [Loading Data into BigQuery](#loading-data-into-bigquery)\n6. [Writing Data to BigQuery](#writing-data-to-bigquery)\n7. [Scheduled Queries with Python](#scheduled-queries-with-python)\n8. [Optimizing Query Performance](#optimizing-query-performance)\n9. [Exporting Data from BigQuery](#exporting-data-from-bigquery)\n10. [Error Handling and Logging](#error-handling-and-logging)\n11. [Cost Management](#cost-management)\n12. [Advanced Use Cases](#advanced-use-cases)\n13. [Integrating with Other GCP Services](#integrating-with-other-gcp-services)\n14. [Security Best Practices](#security-best-practices)\n15. [Conclusion](#conclusion)\n\n---\n\n## Prerequisites \n\nBefore you begin, ensure you have the following:\n- A **Google Cloud Platform (GCP) account** with billing enabled.\n- A **GCP project** with the BigQuery API enabled.\n- **Python 3.7+** installed on your local machine or cloud environment.\n- The **Google Cloud SDK** installed and authenticated:\n  ```bash\n  gcloud auth application-default login\n\nThe **google-cloud-bigquery** and **pandas** libraries installed:\npip install google-cloud-bigquery pandas\n\n\n\n__Setting Up Authentication__ <a name=\"setting-up-authentication\"></a>\nTo interact with BigQuery from Python, you need to authenticate using a service account:\n\n\n## Create a Service Account in GCP:\n\n__Create a new service account__ and assign it the BigQuery Admin role. Navigate to IAM & Admin > Service Accounts.\nGenerate a JSON key file and download it.\n\n\n\n__Set the Environment Variable:__\nimport os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"path/to/your/service-account-key.json\"\n\n\n\n__Connecting to BigQuery__ <a name=\"connecting-to-bigquery\"></a>\nUse the google-cloud-bigquery library to establish a connection:\nfrom google.cloud import bigquery\n\n## Initialize a BigQuery client\n```python\nclient = bigquery.Client()\n```\n## Querying Data from BigQuery \n__Example:__  Analyzing E-Commerce Sales Data\nSuppose you have a dataset containing e-commerce transactions. You want to analyze daily sales trends:\n```python\ndef query_daily_sales():\n    query = \"\"\"\n        SELECT\n            DATE(transaction_time) AS transaction_date,\n            SUM(amount) AS total_sales,\n            COUNT(DISTINCT user_id) AS unique_customers\n        FROM\n            `your_project.your_dataset.ecommerce_transactions`\n        GROUP BY\n            transaction_date\n        ORDER BY\n            transaction_date\n    \"\"\"\n    query_job = client.query(query)  # Run the query\n    results = query_job.result()  # Wait for the query to complete\n\n    for row in results:\n        print(f\"Date: {row.transaction_date}, Sales: \\${row.total_sales}, Customers: {row.unique_customers}\")\n\nquery_daily_sales()\n```\n### Key Points:\n\nUse parameterized queries to avoid SQL injection.\nFor large datasets, use query_job.to_dataframe() to convert results to a Pandas DataFrame for further analysis.\n\n__Example:__  Parameterized Queries\n```python\ndef query_sales_by_date(start_date, end_date):\n    query = \"\"\"\n        SELECT\n            DATE(transaction_time) AS transaction_date,\n            SUM(amount) AS total_sales\n        FROM\n            `your_project.your_dataset.ecommerce_transactions`\n        WHERE\n            DATE(transaction_time) BETWEEN @start_date AND @end_date\n        GROUP BY\n            transaction_date\n        ORDER BY\n            transaction_date\n    \"\"\"\n    job_config = bigquery.QueryJobConfig(\n        query_parameters=[\n            bigquery.ScalarQueryParameter(\"start_date\", \"DATE\", start_date),\n            bigquery.ScalarQueryParameter(\"end_date\", \"DATE\", end_date),\n        ]\n    )\n    query_job = client.query(query, job_config=job_config)\n    results = query_job.result().to_dataframe()\n    return results\n```\n## Usage\n```python\nsales_data = query_sales_by_date(\"2025-01-01\", \"2025-01-31\")\nprint(sales_data.head())\n```\n## Loading Data into BigQuery \n__Example:__ Uploading a CSV File\n\nIf you have a local CSV file (e.g., new_transactions.csv), you can load it into BigQuery:\n```python\ndef load_csv_to_bigquery():\n    table_id = \"your_project.your_dataset.new_transactions\"\n\n    job_config = bigquery.LoadJobConfig(\n        source_format=bigquery.SourceFormat.CSV,\n        skip_leading_rows=1,\n        autodetect=True,\n        write_disposition=\"WRITE_TRUNCATE\"\n    )\n\n    with open(\"new_transactions.csv\", \"rb\") as source_file:\n        job = client.load_table_from_file(\n            source_file, table_id, job_config=job_config\n        )\n\n    job.result()  # Wait for the job to complete\n    print(f\"Loaded {job.output_rows} rows into {table_id}\")\n\nload_csv_to_bigquery()\n```\n## Best Practices:\n\nUse WRITE_TRUNCATE to replace the table or WRITE_APPEND to add data.\nFor large files, consider using Cloud Storage as an intermediate step.\n\n__Example:__ Loading from Pandas DataFrame\n\n```python\nimport pandas as pd\n\ndef load_dataframe_to_bigquery():\n    data = {\n        \"transaction_id\": [\"1001\", \"1002\", \"1003\"],\n        \"user_id\": [\"user1\", \"user2\", \"user3\"],\n        \"amount\": [99.99, 149.99, 199.99]\n    }\n    df = pd.DataFrame(data)\n    table_id = \"your_project.your_dataset.new_transactions_df\"\n\n    job = client.load_table_from_dataframe(\n        df, table_id, job_config=bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n    )\n    job.result()\n    print(f\"Loaded {job.output_rows} rows into {table_id}\")\n\nload_dataframe_to_bigquery()\n```\n## ‚úç Writing Data to BigQuery \nExample: Streaming Real-Time Data\nIf you have real-time data (e.g., from an API), you can stream it into BigQuery:\n```python\ndef stream_real_time_data(rows_to_insert):\n    table_id = \"your_project.your_dataset.real_time_transactions\"\n    table = client.get_table(table_id)\n\n    errors = client.insert_rows(table, rows_to_insert)\n    if errors:\n        print(f\"Encountered errors: {errors}\")\n    else:\n        print(\"Data streamed successfully.\")\n\n# Example data\nrows_to_insert = [\n    {\"transaction_id\": \"1001\", \"user_id\": \"user1\", \"amount\": 99.99},\n    {\"transaction_id\": \"1002\", \"user_id\": \"user2\", \"amount\": 149.99}\n]\n\nstream_real_time_data(rows_to_insert)\n```\n### Note:\n\nStreaming is ideal for low-latency use cases but incurs higher costs.\nFor batch processing, use load_table_from_dataframe or load_table_from_file.\n\n\n## Scheduled Queries with Python\n__Example:__ Automating Daily Reports\nUse Cloud Scheduler and Cloud Functions to run queries on a schedule. Here‚Äôs a Python function for a Cloud Function:\n```python\ndef generate_daily_report(request):\n    client = bigquery.Client()\n    query = \"\"\"\n        SELECT\n            DATE(transaction_time) AS transaction_date,\n            SUM(amount) AS total_sales\n        FROM\n            `your_project.your_dataset.ecommerce_transactions`\n        WHERE\n            DATE(transaction_time) = CURRENT_DATE()\n        GROUP BY\n            transaction_date\n    \"\"\"\n    query_job = client.query(query)\n    results = query_job.result().to_dataframe()\n\n    # Send results via email or save to Cloud Storage\n    print(results)\n    return \"Report generated successfully.\"\n```\n## Deployment:\n\nDeploy this function to Cloud Functions and trigger it daily using Cloud Scheduler.\n\n\n## Optimizing Query Performance & Best Practices\n\nPartition your tables by date or integer ranges to reduce query costs.\nUse clustering for frequently filtered columns.\nAvoid SELECT *‚Äîonly query the columns you need.\nLeverage materialized views for repetitive queries.\n\n__Example:__  Creating a Partitioned Table\n```python\ndef create_partitioned_table():\n    table_id = \"your_project.your_dataset.partitioned_transactions\"\n\n    schema = [\n        bigquery.SchemaField(\"transaction_id\", \"STRING\"),\n        bigquery.SchemaField(\"transaction_time\", \"TIMESTAMP\"),\n        bigquery.SchemaField(\"amount\", \"FLOAT64\")\n    ]\n\n    table = bigquery.Table(table_id, schema=schema)\n    table.time_partitioning = bigquery.TimePartitioning(\n        type_=bigquery.TimePartitioningType.DAY,\n        field=\"transaction_time\"\n    )\n\n    table = client.create_table(table)\n    print(f\"Created partitioned table {table.table_id}\")\n\ncreate_partitioned_table()\n```\n__Example:__  Creating a Clustered Table\n```python\ndef create_clustered_table():\n    table_id = \"your_project.your_dataset.clustered_transactions\"\n\n    schema = [\n        bigquery.SchemaField(\"transaction_id\", \"STRING\"),\n        bigquery.SchemaField(\"user_id\", \"STRING\"),\n        bigquery.SchemaField(\"amount\", \"FLOAT64\")\n    ]\n\n    table = bigquery.Table(table_id, schema=schema)\n    table.clustering_fields = [\"user_id\"]\n\n    table = client.create_table(table)\n    print(f\"Created clustered table {table.table_id}\")\n\ncreate_clustered_table()\n```\n## Exporting Data from BigQuery \n#### __Example:__ Exporting Query Results to CSV\n```python\ndef export_to_csv():\n    query = \"\"\"\n        SELECT * FROM `your_project.your_dataset.ecommerce_transactions`\n        WHERE transaction_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)\n    \"\"\"\n    query_job = client.query(query)\n    results = query_job.result().to_dataframe()\n\n    results.to_csv(\"recent_transactions.csv\", index=False)\n    print(\"Data exported to recent_transactions.csv\")\n\nexport_to_csv()\nExample: Exporting to Cloud Storage\ndef export_to_cloud_storage():\n    destination_uri = \"gs://your-bucket/recent_transactions.avro\"\n    dataset_ref = client.dataset(\"your_dataset\", project=\"your_project\")\n    table_ref = dataset_ref.table(\"ecommerce_transactions\")\n\n    extract_job = client.extract_table(\n        table_ref,\n        destination_uri,\n        location=\"US\"\n    )\n    extract_job.result()\n    print(f\"Exported data to {destination_uri}\")\n\nexport_to_cloud_storage()\n```\n## ‚õëÔ∏è Error Handling and Logging \nAlways include error handling to manage API limits, network issues, and invalid queries:\n```python\nfrom google.api_core.exceptions import GoogleAPICallError, RetryError\n\ndef safe_query(query):\n    try:\n        query_job = client.query(query)\n        return query_job.result()\n    except GoogleAPICallError as e:\n        print(f\"API Error: {e}\")\n    except RetryError as e:\n        print(f\"Retry Error: {e}\")\n    except Exception as e:\n        print(f\"Unexpected Error: {e}\")\n```\n## Cost Management \n\nMonitor usage in the BigQuery UI under Query History.\nSet up alerts for unusual spending in Cloud Billing.\nUse flat-rate pricing for predictable workloads.\nOptimize queries to reduce data scanned.\n\n\n## Advanced Use Cases \n#### __Example:__ Using BigQuery ML\n```python\ndef create_ml_model():\n    query = \"\"\"\n        CREATE OR REPLACE MODEL `your_project.your_dataset.sales_forecast_model`\n        OPTIONS(\n            model_type=ARIMA\n            time_series_timestamp_col=transaction_date\n            time_series_data_col=total_sales\n        ) AS\n        SELECT\n            DATE(transaction_time) AS transaction_date,\n            SUM(amount) AS total_sales\n        FROM\n            `your_project.your_dataset.ecommerce_transactions`\n        GROUP BY\n            transaction_date\n    \"\"\"\n    client.query(query).result()\n    print(\"ML model created successfully.\")\n\ncreate_ml_model()\n```\nExample: Integrating with Dataflow\n# Example Apache Beam pipeline for Dataflow\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\ndef run_dataflow_pipeline():\n    options = PipelineOptions(\n        project=\"your_project\",\n        runner=\"DataflowRunner\",\n        region=\"us-central1\"\n    )\n\n    with beam.Pipeline(options=options) as p:\n        (p\n         | \"Read from BigQuery\" >> beam.io.ReadFromBigQuery(\n             query=\"SELECT * FROM `your_project.your_dataset.ecommerce_transactions`\",\n             use_standard_sql=True\n         )\n         | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n             table=\"your_project.your_dataset.processed_transactions\",\n             schema=\"transaction_id\\:STRING, user_id\\:STRING, amount\\:FLOAT64\",\n             create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n             write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n         )\n        )\n\nrun_dataflow_pipeline()\n```\n\n## Integrating with Other GCP Services \n\n### __Example:__ \n#### Triggering BigQuery from Cloud Storage\n```python\nfrom google.cloud import storage\n\ndef trigger_bigquery_on_new_file(bucket_name, file_name):\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(file_name)\n\n    if blob.exists():\n        load_csv_to_bigquery(f\"gs://{bucket_name}/{file_name}\")\n    else:\n        print(f\"File {file_name} not found in bucket {bucket_name}\")\n\ntrigger_bigquery_on_new_file(\"your-bucket\", \"new_transactions.csv\")\n```\n## üîê Security Best Practices \n- Use IAM roles to grant least privilege access.\n- Encrypt sensitive data using Cloud KMS.\n- Audit logs to monitor access and changes.\n\n\n## Conclusion \nGoogle BigQuery and Python are a powerful combination for data analysis, ETL, and real-time processing. By following the examples and best practices above, you can start building scalable, efficient data pipelines on GCP.",
    "summary": "This guide provides **real-world code examples** and best practices for integrating BigQuery with Python on Google Cloud Platform (GCP).",
    "read_time": "18 min read",
    "tags": "python,cloud,programming,data,sql",
    "category": "Data",
    "created_on": "2025-08-24 03:55:33",
    "updated_on": "2025-08-24 03:55:33",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "24",
    "slug": "go-fiber-ref",
    "title": "Go Fiber HTTP Methods",
    "subtitle": "A list of HTTP methods for the Fiber API",
    "author": "Keith Thomson",
    "content": "# üêÆ Fiber Go Request Methods Reference\n\n## üìã Introduction\n\nThis is a quick reference guide for **Go Fiber HTTP methods** - showing you how to access client request data and send responses.\n\n**What's covered:**\n- üîç General request information\n- üéØ Query parameters and route params\n- üì® Headers and cookies\n- üì¶ Request body handling\n- üì§ Response methods\n- üîÄ Redirects and middleware\n\n---\n\n## üöÄ General Request Info\n\nAccess basic information about the incoming request:\n\n| Method | Returns | Description |\n|--------|---------|-------------|\n| `ctx.Method()` | `[]byte` | HTTP method (GET, POST, etc.) |\n| `ctx.Path()` | `[]byte` | Request path (e.g., `/users/123`) |\n| `ctx.RequestURI()` | `[]byte` | Full request URI |\n| `ctx.Host()` | `[]byte` | Host header |\n| `ctx.RemoteAddr()` | `net.Addr` | Client IP + port |\n| `ctx.RemoteIP()` | `net.IP` | Just the client IP |\n\n---\n\n## üéØ Query Parameters & Route Params\n\n### Query Strings\n\n```go\n// Access query strings (?foo=bar&name=john)\nctx.QueryArgs()                    // *fasthttp.Args - All query args\nctx.QueryArgs().Peek(\"foo\")        // []byte - Get specific parameter\nctx.Query(\"foo\")                   // string - Get query parameter\nctx.Query(\"missing\", \"default\")    // string - With default value\n```\n\n### Route Parameters\n\n```go\n// For routes like /users/:id\nctx.Params(\"id\")                   // string - Get route parameter\nctx.UserValue(\"param\")             // interface{} - Generic parameter access\n```\n\n---\n\n## üì® Headers & Cookies\n\n### Request Headers\n\n```go\n// Access request headers\nctx.Get(\"Header-Name\")             // string - Get header value\nctx.Request.Header.Peek(\"Header\")  // []byte - Raw header access\n\n// Common headers\nctx.Get(\"Content-Type\")            // Get content type\nctx.Get(\"User-Agent\")              // Get user agent\nctx.Get(\"Referer\")                 // Get referer\n```\n\n### Cookies\n\n```go\n// Read cookies\nctx.Cookies(\"cookie-name\")         // string - Get cookie value\nctx.Cookies(\"missing\", \"default\")  // string - With default value\n```\n\n---\n\n## üì¶ Request Body\n\n### Raw Body\n\n```go\n// Access raw request body\nctx.Body()                         // []byte - Raw body content\nctx.BodyParser(&struct)            // error - Parse JSON/XML/Form into struct\n```\n\n### Form Data\n\n```go\n// application/x-www-form-urlencoded\nctx.FormValue(\"key\")               // string - Get form field\nctx.FormValue(\"key\", \"default\")    // string - With default value\n```\n\n### Multipart Forms & File Uploads\n\n```go\n// Handle file uploads\nfile, err := ctx.FormFile(\"upload\")\nif err == nil {\n    ctx.SaveFile(file, \"./uploads/\"+file.Filename)\n}\n\n// Get multipart form\nform, err := ctx.MultipartForm()\nif err == nil {\n    files := form.File[\"documents\"]\n    // Process multiple files\n}\n```\n\n### JSON Body\n\n```go\ntype User struct {\n    Name  string `json:\"name\"`\n    Email string `json:\"email\"`\n}\n\nvar user User\nif err := ctx.BodyParser(&user); err != nil {\n    return ctx.Status(400).JSON(fiber.Map{\n        \"error\": \"Invalid JSON\",\n    })\n}\n```\n\n---\n\n## üì§ Response Methods\n\n### Status Codes\n\n```go\nctx.Status(200)                    // Set status code\nctx.SendStatus(404)                // Send status with default message\n```\n\n### Response Headers\n\n```go\nctx.Set(\"Header-Name\", \"value\")    // Set response header\nctx.Type(\"json\")                   // Set Content-Type\nctx.Type(\"html\", \"utf-8\")          // With charset\n```\n\n### Response Body\n\n```go\n// Send different response types\nctx.SendString(\"Hello World\")      // Plain text\nctx.JSON(fiber.Map{                // JSON response\n    \"message\": \"success\",\n})\nctx.SendFile(\"./public/index.html\") // Serve file\nctx.Download(\"./files/doc.pdf\")    // Force download\nctx.Render(\"template\", fiber.Map{}) // Render template\n```\n\n### Cookies\n\n```go\n// Set response cookies\ncookie := new(fiber.Cookie)\ncookie.Name = \"session\"\ncookie.Value = \"abc123\"\ncookie.Expires = time.Now().Add(24 * time.Hour)\nctx.Cookie(cookie)\n```\n\n---\n\n## üîÄ Redirects\n\n```go\n// Redirect to URL\nctx.Redirect(\"/new-path\")          // 302 redirect (default)\nctx.Redirect(\"/new-path\", 301)     // 301 permanent redirect\n\n// Redirect back\nctx.Redirect(ctx.Get(\"Referer\"))   // Go back to previous page\n```\n\n---\n\n## ‚öôÔ∏è Middleware & Utilities\n\n```go\n// Context methods\nctx.Next()                         // Pass to next handler\nctx.Locals(\"key\", value)           // Set local variable\nctx.Locals(\"key\")                  // Get local variable\n\n// Request timing\nctx.Context().Time()               // Request start time\n\n// Response manipulation\nctx.Response().Reset()             // Clear response\nctx.Request().Reset()              // Clear request\n```\n\n---\n\n## ‚úÖ Complete Example: REST API\n\n### Simple Todo API\n\n```go\npackage main\n\nimport (\n    \"github.com/gofiber/fiber/v2\"\n    \"github.com/gofiber/fiber/v2/middleware/logger\"\n)\n\ntype Todo struct {\n    ID        int    `json:\"id\"`\n    Title     string `json:\"title\"`\n    Completed bool   `json:\"completed\"`\n}\n\nvar todos = []Todo{\n    {ID: 1, Title: \"Learn Fiber\", Completed: false},\n    {ID: 2, Title: \"Build API\", Completed: false},\n}\n\nfunc main() {\n    app := fiber.New()\n\n    // Middleware\n    app.Use(logger.New())\n\n    // API Routes\n    api := app.Group(\"/api\")\n    \n    v1 := api.Group(\"/v1\")\n    \n    // GET all todos\n    v1.Get(\"/todos\", func(c *fiber.Ctx) error {\n        return c.JSON(todos)\n    })\n    \n    // GET single todo\n    v1.Get(\"/todos/:id\", func(c *fiber.Ctx) error {\n        id := c.Params(\"id\")\n        for _, todo := range todos {\n            if fmt.Sprint(todo.ID) == id {\n                return c.JSON(todo)\n            }\n        }\n        return c.Status(404).JSON(fiber.Map{\n            \"error\": \"Todo not found\",\n        })\n    })\n    \n    // POST new todo\n    v1.Post(\"/todos\", func(c *fiber.Ctx) error {\n        var newTodo Todo\n        if err := c.BodyParser(&newTodo); err != nil {\n            return c.Status(400).JSON(fiber.Map{\n                \"error\": \"Invalid request body\",\n            })\n        }\n        newTodo.ID = len(todos) + 1\n        todos = append(todos, newTodo)\n        return c.Status(201).JSON(newTodo)\n    })\n    \n    // PUT update todo\n    v1.Put(\"/todos/:id\", func(c *fiber.Ctx) error {\n        id := c.Params(\"id\")\n        var updated Todo\n        if err := c.BodyParser(&updated); err != nil {\n            return c.Status(400).JSON(fiber.Map{\n                \"error\": \"Invalid request body\",\n            })\n        }\n        \n        for i, todo := range todos {\n            if fmt.Sprint(todo.ID) == id {\n                todos[i].Title = updated.Title\n                todos[i].Completed = updated.Completed\n                return c.JSON(todos[i])\n            }\n        }\n        return c.Status(404).JSON(fiber.Map{\n            \"error\": \"Todo not found\",\n        })\n    })\n    \n    // DELETE todo\n    v1.Delete(\"/todos/:id\", func(c *fiber.Ctx) error {\n        id := c.Params(\"id\")\n        for i, todo := range todos {\n            if fmt.Sprint(todo.ID) == id {\n                todos = append(todos[:i], todos[i+1:]...)\n                return c.SendStatus(204)\n            }\n        }\n        return c.Status(404).JSON(fiber.Map{\n            \"error\": \"Todo not found\",\n        })\n    })\n\n    app.Listen(\":3000\")\n}\n```\n\n---\n\n## üéØ Conclusion\n\nFiber provides a clean, Express-like API for building high-performance web applications in Go.\n\n**Key Takeaways:**\n- üöÄ Fast and efficient request handling\n- üìù Simple, intuitive API\n- üîß Powerful middleware support\n- üåê Express-inspired routing\n\n**Next Steps:**\n- üìö Explore [Fiber documentation](https://docs.gofiber.io/)\n- üîå Try built-in middleware (CORS, compression, etc.)\n- üóÑÔ∏è Integrate with databases\n- üîí Add authentication and authorization\n\n**Happy coding with Fiber!** üéâ",
    "summary": "A list of Methods from the Fiber API",
    "read_time": "2",
    "tags": "go, golang, fiber, http, web,reference",
    "category": "Web Development",
    "created_on": "2025-09-21 08:33:31",
    "updated_on": "2025-09-21 08:33:33",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "25",
    "slug": "docker-nginx-ubuntu",
    "title": "How to Deploy Your Express App with Docker, Nginx, and HTTPS (Ubuntu)",
    "subtitle": "A comprehensive deployment guide for Ubuntu servers",
    "author": "Keith Thomson",
    "content": "# üê≥ How to Deploy Your Express App with Docker, Nginx, and HTTPS (Ubuntu)\n\n> üìò This guide walks you through deploying your Express application on an Ubuntu Droplet, using Docker, Nginx, and Let's Encrypt SSL certificates.\n\n**We will use:**\n- üê≥ **Docker** - To containerize your application\n- üåê **Nginx** - As a reverse proxy to handle HTTPS\n- üîí **Certbot** - For free SSL certificates\n\n---\n\n## ‚úÖ Prerequisites\n\nBefore starting, make sure you have:\n\n1. üñ•Ô∏è **Ubuntu Droplet** - A running Ubuntu server (DigitalOcean, AWS, etc.)\n2. üåç **Domain Name** - e.g., `your-domain.com` with an A record pointing to your server's IP\n3. üìÅ **Application Code** - Your Express app's source code on the server (typically cloned from Git)\n\n---\n\n## üõ†Ô∏è Step 1: Install and Configure Dependencies\n\n### Update your package lists\n\n```bash\nsudo apt-get update\n```\n\n### 1Ô∏è‚É£ Install Nginx\n\n```bash\nsudo apt-get install nginx -y\n```\n\n### 2Ô∏è‚É£ Install Docker\n\nUbuntu's default `docker.io` package works, but installing from the official Docker repo is recommended:\n\n```bash\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io -y\n```\n\n### 3Ô∏è‚É£ Install Certbot and Nginx Plugin\n\n```bash\nsudo apt-get install certbot python3-certbot-nginx -y\n```\n\n### 4Ô∏è‚É£ Start and Enable Services\n\n```bash\nsudo systemctl start docker\nsudo systemctl enable docker\nsudo systemctl start nginx\nsudo systemctl enable nginx\n```\n\n---\n\n## üì¶ Step 2: Build and Run Your Docker App\n\n### Navigate to your app's code\n\n```bash\ngit clone https://github.com/your-repo/my-app.git my-express-app\ncd my-express-app\n```\n\n### Build the Docker image\n\n```bash\ndocker build -t my-express-app .\n```\n\n### Run the container\n\nThis command runs your app in the background, ensures it restarts on failure, and binds it exclusively to `localhost:3000`:\n\n> üîí **Security Note:** Binding to `127.0.0.1:3000` means your app is NOT accessible from the public internet‚Äîonly Nginx on the server can communicate with it.\n\n```bash\ndocker run \\\n  -d \\\n  --restart always \\\n  --name express-app \\\n  -p 127.0.0.1:3000:3000 \\\n  my-express-app\n```\n\n**Verify the container is running:**\n\n```bash\ndocker ps\n```\n\n---\n\n## üåê Step 3: Configure Nginx\n\n### Create Nginx Configuration\n\nCreate a new configuration file for your domain:\n\n```bash\nsudo nano /etc/nginx/sites-available/your-domain.com\n```\n\n**Add this configuration:**\n\n```nginx\nserver {\n    listen 80;\n    server_name your-domain.com www.your-domain.com;\n\n    location / {\n        proxy_pass http://127.0.0.1:3000;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header Host $host;\n        proxy_cache_bypass $http_upgrade;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n```\n\n### Enable the site\n\nCreate a symbolic link to enable the configuration:\n\n```bash\nsudo ln -s /etc/nginx/sites-available/your-domain.com /etc/nginx/sites-enabled/\n```\n\n### (Recommended) Remove default Nginx config\n\n```bash\nsudo rm /etc/nginx/sites-enabled/default\n```\n\n### Test Nginx configuration\n\n```bash\nsudo nginx -t\n```\n\n### If successful, restart Nginx\n\n```bash\nsudo systemctl restart nginx\n```\n\n**Test your setup:** Visit `http://your-domain.com` - you should see your Express app (without HTTPS yet).\n\n---\n\n## üîí Step 4: Enable HTTPS with Certbot\n\n### Run Certbot\n\nThis command will automatically configure SSL for your domain:\n\n```bash\nsudo certbot --nginx\n```\n\n**Follow the prompts:**\n1. üìß Enter your email address\n2. ‚úÖ Agree to the terms of service\n3. üåê Select your domain from the list\n4. ‚ôªÔ∏è Choose to redirect HTTP to HTTPS (recommended)\n\n### Verify SSL certificate\n\nCertbot will automatically configure Nginx and obtain a free SSL certificate. Visit:\n\n```\nhttps://your-domain.com\n```\n\nYou should see:\n- ‚úÖ A valid SSL certificate\n- üîí Your Express app running securely over HTTPS\n\n---\n\n## üîÑ Auto-Renewal\n\nCertbot sets up automatic renewal via a systemd timer. You can test the renewal process:\n\n```bash\nsudo certbot renew --dry-run\n```\n\nTo check renewal timer status:\n\n```bash\nsudo systemctl status certbot.timer\n```\n\n---\n\n## üéØ Useful Docker Commands\n\n### View container logs\n\n```bash\ndocker logs express-app\n```\n\n### Restart the container\n\n```bash\ndocker restart express-app\n```\n\n### Stop the container\n\n```bash\ndocker stop express-app\n```\n\n### Remove the container\n\n```bash\ndocker rm express-app\n```\n\n### Rebuild and redeploy\n\n```bash\ndocker stop express-app\ndocker rm express-app\ndocker build -t my-express-app .\ndocker run -d --restart always --name express-app -p 127.0.0.1:3000:3000 my-express-app\n```\n\n---\n\n## üéâ You're Done!\n\nYour Express application is now:\n- üê≥ Containerized with Docker\n- üåê Proxied through Nginx\n- üîí Secured with HTTPS\n- üîÑ Automatically renewed SSL certificates\n- ‚ôªÔ∏è Auto-restarting on failure\n\n**Congratulations on your production deployment!** üöÄ",
    "summary": "A comprehensive guide to deploying Express applications on Ubuntu using Docker, Nginx as a reverse proxy, and HTTPS with Certbot for secure web application deployment.",
    "read_time": "10 min read",
    "tags": "docker,nginx,ubuntu,express,https,certbot,deployment,devops",
    "category": "DevOps",
    "created_on": "2025-11-03 12:00:00",
    "updated_on": "2025-11-03 12:00:00",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "26",
    "slug": "go-static-generator",
    "title": "Building a static HTML generator with Go",
    "subtitle": "Dynamically generate blog posts using Go's html/template package",
    "author": "Keith Thomson",
    "content": "# üèóÔ∏è Building a Static HTML Generator with Go\n\n## üìã Introduction\n\nThis guide explains how to generate **static HTML pages** using Go's standard html/template package.\n\n**What you'll learn:**\n- üìÇ Setting up project structure\n- üîÑ Reading JSON data\n- üìù Parsing HTML templates\n- üíæ Writing individual HTML files\n- üöÄ Running the generator\n\n---\n\n## üìÅ Project Structure\n\nYour project should have a templates folder with base.html and post.html files, a data folder with your JSON file, and a generate.go file that will create the output in a generated folder.\n\n---\n\n## üìä JSON Data Format\n\nYour JSON data should contain an array of post objects with fields like id, title, subtitle, summary, readtime, tags, and content.\n\n---\n\n## üìÑ Template Files\n\nCreate two template files:\n\n**base.html** - The main layout template with header, footer, and a content block placeholder.\n\n**post.html** - Defines the content block with article structure including title, subtitle, read time, and content.\n\n---\n\n## üíª Core Generator Script\n\nThe generate.go file contains:\n\n**Post struct** - Defines the data structure for a blog post with ID, Title, Subtitle, Summary, ReadTime, Tags, and Content fields.\n\n**loadPosts function** - Reads the JSON file and unmarshals it into a slice of Post structs.\n\n**renderTemplate function** - Takes a post and template, creates an HTML file in the generated directory, and executes the template with the post data.\n\n**main function** - Loads posts, parses templates, and renders each post to an HTML file.\n\n---\n\n## üîç Key Functions Explained\n\n### loadPosts Function\n\n**Purpose:** Reads and parses the JSON data file\n\n- üìñ Opens and reads the JSON file\n- üîÑ Deserializes JSON into Go structs\n- ‚úÖ Returns a slice of posts for processing\n- ‚ùå Returns an error if file reading or parsing fails\n\n### renderTemplate Function\n\n**Purpose:** Generates HTML file for each post\n\n- üìù Takes a post and the parsed template set\n- üìÅ Creates an output file under /generated/\n- üîÑ Executes the base template injecting the post data\n- üé® Template blocks allow content override\n\n---\n\n## üöÄ Running the Generator\n\nExecute the generator with:\n\n**go run generate.go**\n\nAll static files are written to the /generated directory and can be served using any static web server.\n\n---\n\n## üé® Extending the Generator\n\nYou could enhance this with:\n\n### 1. üìë Index Page\n- Generate a homepage listing all posts\n- Add pagination for large post collections\n\n### 2. üìù Markdown Support\n- Convert Markdown to HTML using goldmark or blackfriday\n- Write posts in Markdown instead of HTML\n\n### 3. ‚ö° Performance\n- Implement template caching\n- Add incremental builds for changed files only\n- Parallelize rendering with goroutines\n\n### 4. üì° RSS/Atom Feeds\n- Generate RSS or Atom feeds for blog subscriptions\n- Include metadata and timestamps\n\n### 5. üéØ Enhanced Features\n- Add syntax highlighting for code blocks\n- Generate sitemaps for SEO\n- Create tag and category pages\n\n---\n\n## üîÑ Auto-Regeneration with Air\n\nFor development, automatically regenerate on file changes.\n\n**Install Air:**\n\ngo install github.com/air-verse/air@latest\n\n**Create .air.toml configuration** with settings for root directory, build command, file extensions to watch, and directories to exclude.\n\n**Run Air:** Simply run the air command and it will re-run generate.go on any change, making development instant!\n\n---\n\n## üìö Additional Resources\n\n- üìò Go html/template documentation at pkg.go.dev\n- üåê Goldmark Markdown parser on GitHub\n- üî• Air live reload tool on GitHub\n\n---\n\n## üéØ Conclusion\n\nYou now have a working static site generator in Go! This is a great foundation for:\n\n- üìù Personal blogs\n- üìö Documentation sites\n- üåê Marketing pages\n- üìä Portfolio websites\n\n**Happy generating!** üöÄ",
    "summary": "A simple example of a static HTML generator.",
    "read_time": "14 min read",
    "tags": "go,webdev,programming",
    "category": "DevOps",
    "created_on": "2025-11-08 12:00:00",
    "updated_on": "2025-11-08 12:00:00",
    "published": "1",
    "featured": "1"
  }
]